# backend/app/services/llm.py
# OpenRouter LLM client â€” async httpx client for chat completions with structured output
# Handles retries, timeouts, streaming, and model listing
# Related: config.py, models/schemas.py
